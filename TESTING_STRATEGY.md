# Testing Strategy: AgentKit Python Module

This document outlines the testing strategies for the `agentkit` Python module.

## Task 4.1: Enhance Unit Test Coverage (>80%)

This section outlines the plan for **Task 4.1: Enhance unit test coverage** for the `agentkit` Python module, aiming for >80% coverage and focusing on edge cases and failure modes.

### Plan Steps

1.  **Integrate Coverage Tool:**
    *   Add `pytest-cov` to `requirements.txt`.
    *   Run an initial coverage report (`pytest --cov=agentkit tests/`) to establish the baseline coverage percentage and identify untested areas.

2.  **Analyze Coverage Report:**
    *   Review the report generated by `pytest-cov`.
    *   Pinpoint specific modules, functions, classes, and code branches within `agentkit/` that lack sufficient test coverage.

3.  **Prioritize Test Implementation:**
    *   Focus initially on core logic modules:
        *   `agentkit/registration/`
        *   `agentkit/messaging/`
        *   `agentkit/tools/`
    *   Then move to interface layers:
        *   `agentkit/api/endpoints/`
        *   `agentkit/sdk/`
        *   `agentkit/cli/`
    *   Finally, cover supporting modules:
        *   `agentkit/core/models.py` (especially validation)
        *   `agentkit/api/middleware.py`

4.  **Implement New Unit Tests (Iterative):**
    *   For each targeted module/file, add tests in the corresponding `tests/` directory (creating new test files if necessary).
    *   **Focus Areas:**
        *   **Happy Paths:** Ensure core functionality works as expected with valid inputs.
        *   **Edge Cases:** Test boundary conditions, empty inputs, zero values, maximum limits, etc.
        *   **Failure Modes/Error Handling:** Test invalid inputs (wrong types, missing fields), expected exceptions (e.g., AgentNotFound, ToolNotFound), error responses from API endpoints, schema validation failures.
        *   **Validation Logic:** Explicitly test Pydantic model validation and any custom validation logic.
        *   **Mocking:** Use `unittest.mock` appropriately to isolate units, especially for SDK tests (mocking `requests`/`httpx`) and CLI tests (mocking SDK calls).

5.  **Track Progress:**
    *   Regularly re-run the coverage report (`pytest --cov=agentkit tests/`) after adding tests for each module to monitor progress towards the >80% goal.

6.  **Refactor for Testability (If Necessary):**
    *   If certain code sections are difficult to test, consider minor refactoring to improve modularity and testability, adhering to project guidelines.

### Target Modules and Test Locations

```mermaid
graph TD
    subgraph agentkit [agentkit Module (Source)]
        direction LR
        API[api/endpoints/*.py]
        REG[registration/*.py]
        MSG[messaging/*.py]
        TOOL[tools/*.py]
        SDK[sdk/client.py]
        CLI[cli/main.py]
        CORE[core/models.py]
        MW[api/middleware.py]
    end

    subgraph tests [tests Directory (Tests)]
        direction LR
        TestAPI[api/endpoints/test_*.py]
        TestREG[registration/test_*.py]
        TestMSG[messaging/test_*.py]  # Might need creation/enhancement
        TestTOOL[tools/test_*.py]
        TestSDK[sdk/test_client.py]
        TestCLI[cli/test_main.py]      # Might need creation/enhancement
        TestCORE[core/test_models.py]    # Might need creation/enhancement
        TestMW[api/test_middleware.py] # Might need creation/enhancement
    end

    API --> TestAPI
    REG --> TestREG
    MSG --> TestMSG
    TOOL --> TestTOOL
    SDK --> TestSDK
    CLI --> TestCLI
    CORE --> TestCORE
    MW --> TestMW

    style agentkit fill:#lightblue,stroke:#333
    style tests fill:#lightgreen,stroke:#333
```

---

## Task 4.2: Develop Integration Tests

This section outlines the plan for **Task 4.2: Develop integration tests** that simulate full workflows (registration → messaging → tool invocation).

### Plan Steps

1.  **Define Key Workflow Scenarios:**
    *   **Scenario 1 (Basic Registration & Messaging):**
        *   Register Agent A using the SDK.
        *   Register Agent B using the SDK.
        *   Agent A sends a simple message (e.g., type `ping`) to Agent B via the SDK.
        *   Assert that the API response indicates success (or simulates Agent B processing it).
    *   **Scenario 2 (Simple Tool Invocation):**
        *   Register Agent A.
        *   Register a *Mock Tool* (details in Step 2).
        *   Agent A sends a message intended to invoke the Mock Tool via the SDK.
        *   Assert that the API response indicates the tool was invoked (or returns data from the mock tool).
        *   *(Optional)* Assert that the Mock Tool service received the expected request.
    *   **Scenario 3 (Tool Invocation - Non-existent Tool):**
        *   Register Agent A.
        *   Agent A sends a message attempting to invoke a tool that is *not* registered.
        *   Assert that the API returns an appropriate error (e.g., ToolNotFound error via SDK).
    *   **Scenario 4 (Messaging - Non-existent Agent):**
        *   Register Agent A.
        *   Agent A sends a message to a non-existent Agent ID.
        *   Assert that the API returns an appropriate error (e.g., TargetAgentNotFound error via SDK).

2.  **Set Up Test Environment (Docker Compose):**
    *   Utilize the existing `docker-compose.yml` to run the main AgentKit API service in an isolated container.
    *   *(Optional but Recommended)* Consider adding a simple, separate "Mock Tool" service to the `docker-compose.yml`. This service would expose a basic HTTP endpoint that the AgentKit API can call during Scenario 2. This avoids mocking tool calls *within* the API service itself, providing a more realistic integration test.
    *   Ensure tests can reliably determine the API endpoint (e.g., `http://localhost:PORT`) exposed by Docker Compose.

3.  **Test Structure (`tests/integration/`):**
    *   Create a new directory: `tests/integration/`.
    *   Create a test file: `tests/integration/test_workflows.py`.
    *   Use `pytest` fixtures to manage dependencies:
        *   A fixture to ensure the Docker Compose environment is running before tests start and potentially stopped/cleaned up afterward.
        *   A fixture to initialize the `AgentKitClient` pointing to the API service running in Docker.

4.  **Test Implementation (`pytest` + SDK):**
    *   Import `AgentKitClient` and necessary models.
    *   Use the SDK client (provided by a fixture) to perform actions: `register_agent`, `send_message`.
    *   Use standard `pytest` assertions (`assert`) to check:
        *   Successful agent IDs returned upon registration.
        *   Expected success/data responses from `send_message`.
        *   Expected `AgentKitError` exceptions raised by the SDK for error scenarios (e.g., non-existent agent/tool).
    *   If a Mock Tool service is used, tests might need a way to query its state or logs to confirm invocation (this adds complexity but increases test fidelity).

### Conceptual Test Setup Diagram

```mermaid
graph TD
    subgraph Test Runner (pytest)
        direction LR
        TestCode[tests/integration/test_workflows.py] --- FixtureClient[SDK Client Fixture]
        FixtureClient --- SDK[AgentKit SDK Client]
        TestCode --- FixtureDocker[Docker Compose Fixture]
    end

    subgraph Docker Compose Environment
        direction TB
        APIService[AgentKit API Service (FastAPI)] --- Storage[(In-Memory Storage)]
        MockToolService[Mock Tool Service (Optional)]
    end

    FixtureDocker -- Manages --> Docker Compose Environment
    SDK -- HTTP API Calls --> APIService
    APIService -- HTTP Call (if tool invoked) --> MockToolService

    style Test Runner fill:#lightgreen,stroke:#333
    style Docker Compose Environment fill:#lightblue,stroke:#333